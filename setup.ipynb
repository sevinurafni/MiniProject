{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p60DzGIP3-Y2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCaMRiMfPJtN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-2.7.1-cp310-cp310-win_amd64.whl (216.1 MB)\n",
            "     -------------------------------------- 216.1/216.1 MB 4.2 MB/s eta 0:00:00\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.22.1-cp310-cp310-win_amd64.whl (1.7 MB)\n",
            "     ---------------------------------------- 1.7/1.7 MB 6.8 MB/s eta 0:00:00\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
            "     -------------------------------------- 199.1/199.1 KB 4.0 MB/s eta 0:00:00\n",
            "Collecting networkx\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "     ---------------------------------------- 1.7/1.7 MB 5.8 MB/s eta 0:00:00\n",
            "Collecting sympy>=1.13.3\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "     ---------------------------------------- 6.3/6.3 MB 5.8 MB/s eta 0:00:00\n",
            "Collecting filelock\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Collecting jinja2\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "     -------------------------------------- 134.9/134.9 KB 4.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.14.0)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
            "     ---------------------------------------- 12.9/12.9 MB 6.0 MB/s eta 0:00:00\n",
            "Collecting pillow!=8.3.*,>=5.3.0\n",
            "  Downloading pillow-11.2.1-cp310-cp310-win_amd64.whl (2.7 MB)\n",
            "     ---------------------------------------- 2.7/2.7 MB 5.9 MB/s eta 0:00:00\n",
            "Collecting mpmath<1.4,>=1.1.0\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "     -------------------------------------- 536.2/536.2 KB 4.8 MB/s eta 0:00:00\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
            "Installing collected packages: mpmath, sympy, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision\n",
            "Successfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.5.1 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 pillow-11.2.1 sympy-1.14.0 torch-2.7.1 torchvision-0.22.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
            "You should consider upgrading via the 'c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision\n",
        "pip install matplotlib\n",
        "pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json\n",
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn, torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup dataset\n",
        "data_dir = \"dataset/\"  # ganti sesuai lokasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-zHmuJe6Tv6",
        "outputId": "807e6681-638a-40ba-9626-32a1ed542927"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: loss 1.7009, acc 0.4744\n",
            "Epoch 2: loss 0.5967, acc 0.8249\n",
            "Epoch 3: loss 0.2467, acc 0.9556\n",
            "Epoch 4: loss 0.0885, acc 0.9935\n",
            "Epoch 5: loss 0.0398, acc 0.9991\n",
            "Selesai training ✅\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
        "train_size = int(0.8*len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=32)\n",
        "\n",
        "# Save mapping\n",
        "class_mapping = {v:k for k,v in dataset.class_to_idx.items()}\n",
        "with open(\"class_mapping.json\", \"w\") as f:\n",
        "    json.dump(class_mapping, f)\n",
        "\n",
        "# Build model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "# Training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "        loss.backward(); optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        correct += (out.argmax(1)==labels).sum().item()\n",
        "    acc = correct/len(train_ds)\n",
        "    print(f\"Epoch {epoch+1}: loss {total_loss/len(train_loader):.4f}, acc {acc:.4f}\")\n",
        "\n",
        "# Save\n",
        "torch.save(model.cpu().state_dict(), \"fish_freshness_model.pth\")\n",
        "print(\"Selesai training ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnQqFjtELu2n",
        "outputId": "21553951-f6e1-40e1-de6c-6f0360f6d4bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Akurasi di data validasi: 0.7711\n"
          ]
        }
      ],
      "source": [
        "# Evaluasi akurasi di data validasi\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        out = model(imgs)\n",
        "        pred = out.argmax(1)\n",
        "        correct += (pred == labels).sum().item()\n",
        "\n",
        "val_acc = correct / len(val_ds)\n",
        "print(f\"Akurasi di data validasi: {val_acc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
